Полное руководство по артефактам работы и А/Б тестированию решений ML-Engineer'a

---
### Артефакты ML-инженера

Цель артефактов — сделать систему воспроизводимой (reproducible), управляемой (operable) и объяснимой (auditable) без “знания в голове автора”. Для бигтеха это означает: любой новый инженер должен уметь поднять обучение/инференс, понять допущения и безопасно раскатить изменения.

**1) Артефакты модели и данных**

- **Model Registry запись**: версия модели (tag), ссылка на commit, параметры обучения, контрольные метрики, дата, владелец, статус (staging/prod/archived).
    
- **Dataset/Feature snapshot**: версия датасета (например, DVC/таблица со snapshot-id), схема фичей, правила фильтрации, “окна” (train/val/test по времени), список исключений.
    
- **Training recipe**: конфиг обучения (YAML/JSON), фиксированные сиды, описание аугментаций/семплинга, стратегия negative mining (для retrieval/ranking).
    

**2) Артефакты пайплайна**

- **Feature pipeline**: код/SQL для получения фичей (batch/stream), SLA по обновлению, контракт схемы, проверки качества данных (null-rate, диапазоны, уникальности).
    
- **Inference pipeline**: препроцессинг → модель → постпроцессинг → бизнес-правила; обязательно отделять “модельный скор” от правил/фильтров.
    
- **Backfill/Replay job**: возможность прогнать модель на исторических данных (для оффлайн-оценки, дебага, инцидентов).
    

**3) Артефакты эксплуатации (SRE/прод)**

- **Runbook**: “что делать если” (высокая латентность, деградация качества, отвал фичей, рост ошибок).
    
- **Monitoring/Alerting**:
    
    - технические: p95/p99 latency, error rate, timeouts, нагрузка, очередь;
        
    - данные: drift, доли null/unknown, смена распределений;
        
    - качество: online proxy (CTR/CR), delayed labels (через разметку/фидбек).
        
- **Release artifacts**: changelog, риск-оценка, план отката, “guardrail” пороги.
    

**4) Артефакты качества и объяснимости**

- **Model Card**: где работает/не работает, срезы (сегменты), bias/risks, ограничения.
    
- **Decision log (RFC)**: почему выбрали подход, альтернативы, что пробовали и что не взлетело.
    
- **Debug toolkit**: ноутбук/скрипты для разборов примеров (top false positives/false negatives, hard cases).
    

---

### Типы A/B и где они применяются

Экспериментальный дизайн зависит от того, **что именно меняется** (UI, ранжирование, цена, модель), и **как устроена среда** (онлайн-пользователи vs оффлайн-точки).

**1) Online продукты (web/app/маркетплейсы)**

- **Классический A/B**: рандомизация по user_id/device_id; хорошо, когда воздействие индивидуально и нет сильных сетевых эффектов.
    
- **A/B/n**: несколько вариантов; почти всегда требует контроля множественных проверок (иначе много ложноположительных).
    
- **Interleaving** (для ранжирования): смешивание результатов двух ранкеров в одной выдаче, чтобы быстрее поймать разницу при меньшем трафике (очень полезно для поиска/рекомендаций).
    
- **Switchback / time-based**: переключение по времени (час A, час B), когда есть сетевые эффекты и “перетекание” между группами (доставка/такси/маркетплейсы).
    

**2) Offline продукты (ритейл, рестораны, логистика)**  
В оффлайне сложнее обеспечить независимость и мощность теста из-за малого числа единиц эксперимента (магазинов/точек) и сильной сезонности; часто используют парное матчинговое сравнение и временные модели.[habr+1](https://habr.com/ru/companies/lentatech/articles/858890/)​

- **Unit of randomization**: магазин/регион/город (а не чек/покупатель внутри магазина).
    
- **Matched pairs**: подбираются “похожие” магазины (трафик, выручка, площадь), один в тест, один в контроль.
    
- **Synthetic control / causal impact**: строится “синтетический контроль” как взвешенная комбинация других точек, чтобы лучше учитывать тренды и сезонность.
    

---

### Метрики: бизнес, продукт, ML, технические

Правильная практика — заранее строить **иерархию метрик**: North Star → драйверы → прокси → guardrails. Это снижает риск “улучшили CTR, но убили прибыль” или “улучшили качество модели, но ухудшили задержки”.

**1) Business (North Star)**

- GMV/Revenue/Margin, Retention, LTV, CAC payback.
    
- Важны бизнесу, но часто шумные и медленно реагируют.
    

**2) Product / Proxy (быстрые драйверы)**

- CTR, Add-to-cart rate, Conversion rate, Search success rate, dwell time.
    
- В RAG/поиске: доля “answered queries”, “zero results rate”, “query reformulation rate”.
    

**3) ML/IR метрики (оффлайн качество модели)**

- Для ретрива: Recall@K, MRR, nDCG@K, HitRate.
    
- Для ранкера: nDCG@K, MAP, pairwise accuracy, calibration (если скор интерпретируется как вероятность).
    

**4) Technical / Guardrails**

- Latency p95/p99, error rate, timeouts, crashes, GPU/CPU, стоимость запроса, размер индекса/векторов.
    
- Guardrails формулируются как “не хуже X”: например, p99 latency не выше +10%, error rate не выше 0.1 п.п.
    

**Пример иерархии для поиска/ранжирования**

- North Star: GMV per search session.
    
- Drivers: orders per session × AOV.
    
- Proxy: CTR@topK, add-to-cart, query success.
    
- ML: nDCG@10 (ранкер), Recall@100 (ретрив).
    
- Guardrails: p99 latency, timeouts, 5xx.
    

---

### Стат для ML, A/B: гипотезы, значимость, мощность

Задача статистики — ответить: “наблюдаемый эффект реален или это шум?”, и “сколько данных нужно, чтобы заметить эффект заданного размера?”.

#### 1) Формулировка гипотез

- **H0H0**: изменений нет (разница = 0).
    
- **H1H1**: изменения есть (двусторонняя) или улучшение (односторонняя — осторожно, заранее фиксировать).
    
- Ошибки:
    
    - Type I (αα): ложноположительная (обычно 0.05).
        
    - Type II (ββ): ложноотрицательная; мощность 1−β1−β обычно 0.8–0.9.
        

#### 2) Базовые тесты по типу метрики

**A) Доли / конверсии (CR, CTR)**

- Часто используют z-test для двух пропорций или биномиальные модели.
    
- Пример: конверсия A = 5.0%, B = 5.3% — проверяем значимость разницы.
    

**B) Средние (time spent, средний чек, latency)**

- t-test при “примерно нормальном” распределении среднего (или больших выборках).
    
- Но для денег/чека часто тяжёлые хвосты → лучше:
    
    - лог-трансформация,
        
    - winsorization,
        
    - бутстрап разницы средних/медиан.
        

**C) “Тяжёлые” метрики и ratio-метрики**

- Пример ratio: ARPU = revenue/users, CTR = clicks/impressions.
    
- Правильнее тестировать через бутстрап или delta method (для дисперсии отношения), иначе легко ошибиться.
    

#### 3) Доверительные интервалы (CI) вместо “p-value магии”

Практика бигтеха: показывать **эффект + CI**.

- Пример формулировки: “uplift +1.2% (95% CI: +0.2…+2.1), p=0.03”.
    
- Если CI пересекает 0 — эффект статистически не подтверждён при выбранном уровне.
    

#### 4) MDE и расчёт длительности эксперимента

- **MDE (Minimum Detectable Effect)**: минимальный эффект, который хотим уметь обнаруживать.
    
- Если трафика мало, то тест с MDE=0.5% может быть бессмысленным: он будет длиться слишком долго и всё равно будет нестабилен.
    
- Практика: фиксировать MDE до запуска и согласовывать с бизнесом: “мы готовы считать успехом +2% к CR, меньше — неважно”.
    

#### 5) SRM (Sample Ratio Mismatch)

Проверка, что сплит не сломан: ожидали 50/50, получили 55/45 — значит, могли “потечь” идентификаторы, фильтрация, логирование.

- Типично проверяют тестом согласия (хи‑квадрат) и останавливают эксперимент при SRM.
    

#### 6) Multiple testing и “подглядывание”

- **A/B/n** и много метрик → растёт шанс ложноположительных → нужны поправки (Bonferroni/Holm/FDR).
    
- **Peeking** (каждый день смотреть p-value и останавливать, когда стало <0.05) раздувает Type I ошибку; решение: фиксированный горизонт или sequential testing.
    

#### 7) Variance reduction: CUPED

Если есть “предэкспериментальная” метрика (например, revenue за прошлую неделю), её можно использовать как ковариату и снизить дисперсию эффекта → быстрее поймать сигнал.

- Смысл: сравниваем не сырые значения, а значения “скорректированные” по базовому уровню.
    

---

### Сквозной пример (поиск/ранжирование) — от гипотезы до решения

**Ситуация:** меняем reranker в поиске (BERT → более лёгкий distilled, но с новым обучением).

1. **Гипотеза**
    

- H1H1: новый reranker увеличит nDCG@10 оффлайн и даст рост Search CTR@10 онлайн без ухудшения p99 latency.
    

2. **Выбор метрик**
    

- Primary: CTR@10 (proxy), Search conversion.
    
- Secondary: GMV/session (если хватает мощности).
    
- Guardrails: p99 latency, timeouts, error rate.
    
- Diagnostic: nDCG@10 оффлайн, Recall@100 ретривера (чтобы не маскировать проблему retrieval-стадии).
    

3. **Дизайн**
    

- Online A/B 50/50, unit=user_id, срок по power analysis.
    
- SRM check ежедневно.
    
- Ограничить rollout: сначала 5% трафика (canary), затем 50%.
    

4. **Статистика**
    

- CTR: тест пропорций (или бутстрап по пользователю).
    
- Revenue: бутстрап (из-за heavy tail) + отчёт CI.
    
- Несколько метрик → контролируем множественные проверки.
    

5. **Интерпретация**
    

- Если CTR вырос, но GMV не изменился: возможно, клики стали “пустыми” → анализ downstream (add-to-cart, purchase).
    
- Если оффлайн nDCG вырос, а онлайн CTR не вырос: вероятны bias/дрейф, mismatch логов, проблемы латентности или UI.
    
