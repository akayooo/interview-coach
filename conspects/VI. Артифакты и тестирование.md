Полное руководство по артефактам работы и А/Б тестированию решений ML-Engineer'a

---

### Артефакты ML-инженера

Цель артефактов — сделать систему воспроизводимой (reproducible), управляемой (operable) и объяснимой (auditable) без “знания в голове автора”. Для бигтеха это означает: любой новый инженер должен уметь поднять обучение/инференс, понять допущения и безопасно раскатить изменения.

**1) Артефакты модели и данных**

* **Model Registry запись**: версия модели (tag), ссылка на commit, параметры обучения, контрольные метрики, дата, владелец, статус (staging/prod/archived).

* **Dataset/Feature snapshot**: версия датасета (например, DVC/таблица со snapshot-id), схема фичей, правила фильтрации, “окна” (train/val/test по времени), список исключений.

* **Training recipe**: конфиг обучения (YAML/JSON), фиксированные сиды, описание аугментаций/семплинга, стратегия negative mining (для retrieval/ranking).

**2) Артефакты пайплайна**

* **Feature pipeline**: код/SQL для получения фичей (batch/stream), SLA по обновлению, контракт схемы, проверки качества данных (null-rate, диапазоны, уникальности).

* **Inference pipeline**: препроцессинг → модель → постпроцессинг → бизнес-правила; обязательно отделять “модельный скор” от правил/фильтров.

* **Backfill/Replay job**: возможность прогнать модель на исторических данных (для оффлайн-оценки, дебага, инцидентов).

**3) Артефакты эксплуатации (SRE/прод)**

* **Runbook**: “что делать если” (высокая латентность, деградация качества, отвал фичей, рост ошибок).

* **Monitoring/Alerting**:

  * технические: p95/p99 latency, error rate, timeouts, нагрузка, очередь;

  * данные: drift, доли null/unknown, смена распределений;

  * качество: online proxy (CTR/CR), delayed labels (через разметку/фидбек).

* **Release artifacts**: changelog, риск-оценка, план отката, “guardrail” пороги.

**4) Артефакты качества и объяснимости**

* **Model Card**: где работает/не работает, срезы (сегменты), bias/risks, ограничения.

* **Decision log (RFC)**: почему выбрали подход, альтернативы, что пробовали и что не взлетело.

* **Debug toolkit**: ноутбук/скрипты для разборов примеров (top false positives/false negatives, hard cases).

#### Вопрос: Что именно должно быть в записи Model Registry, чтобы релиз был воспроизводимым и безопасным?

**Ответ:**
Минимальный стандарт для воспроизводимости — это связка **версия модели (tag) → конкретный commit → точный training recipe (конфиг, сиды, аугментации, семплинг/negative mining)** плюс указание **версии данных/фичей (snapshot-id)**. Для безопасности релиза туда же добавляют **контрольные метрики** (оффлайн и, если есть, онлайн-прокси), **владельца**, **дату**, и **статус** (staging/prod/archived), чтобы было понятно, какая версия разрешена к использованию. Практически важно, чтобы по одной записи можно было: (1) поднять обучение, (2) повторить инференс, (3) объяснить, почему модель “такая”, (4) откатиться на предыдущий tag без ручного поиска “какой же был конфиг”.

#### Вопрос: Зачем явно отделять “модельный скор” от бизнес-правил в Inference pipeline и как это помогает в дебаге?

**Ответ:**
Разделение “модельный скор” и правил/фильтров предотвращает ситуацию, когда улучшение/ухудшение метрик приписывают модели, хотя реальная причина — в **постпроцессинге**, фильтрации, лимитах, правилах ранжирования или бизнес-ограничениях. В дебаге это позволяет локализовать проблему: если скор модели стабилен, а итоговая выдача деградирует, то искать нужно в правилах, данных или инфраструктуре. Кроме того, при A/B тестах и инцидентах проще делать сравнение “скор vs финальный результат”: можно понять, где именно происходит “перелом” (на препроцессинге, на модели, на постпроцессинге или на бизнес-правилах).

---

### Типы A/B и где они применяются

Экспериментальный дизайн зависит от того, **что именно меняется** (UI, ранжирование, цена, модель), и **как устроена среда** (онлайн-пользователи vs оффлайн-точки).

**1) Online продукты (web/app/маркетплейсы)**

* **Классический A/B**: рандомизация по user_id/device_id; хорошо, когда воздействие индивидуально и нет сильных сетевых эффектов.

* **A/B/n**: несколько вариантов; почти всегда требует контроля множественных проверок (иначе много ложноположительных).

* **Interleaving** (для ранжирования): смешивание результатов двух ранкеров в одной выдаче, чтобы быстрее поймать разницу при меньшем трафике (очень полезно для поиска/рекомендаций).

* **Switchback / time-based**: переключение по времени (час A, час B), когда есть сетевые эффекты и “перетекание” между группами (доставка/такси/маркетплейсы).

**2) Offline продукты (ритейл, рестораны, логистика)**
В оффлайне сложнее обеспечить независимость и мощность теста из-за малого числа единиц эксперимента (магазинов/точек) и сильной сезонности; часто используют парное матчинговое сравнение и временные модели.[habr+1](https://habr.com/ru/companies/lentatech/articles/858890/)

* **Unit of randomization**: магазин/регион/город (а не чек/покупатель внутри магазина).

* **Matched pairs**: подбираются “похожие” магазины (трафик, выручка, площадь), один в тест, один в контроль.

* **Synthetic control / causal impact**: строится “синтетический контроль” как взвешенная комбинация других точек, чтобы лучше учитывать тренды и сезонность.

#### Вопрос: Когда interleaving предпочтительнее классического A/B для ранжирования и какой сигнал он даёт быстрее?

**Ответ:**
Interleaving особенно полезен, когда вы сравниваете **два ранкера** (или две версии ранжирования) и хотите быстро получить чувствительный сигнал при ограниченном трафике. Вместо того чтобы “разводить” пользователей по группам, interleaving **смешивает результаты** в одной выдаче и наблюдает, на какие позиции и документы пользователи кликают. Это часто быстрее выявляет разницу в “предпочтении” ранкера по кликам, потому что сравнение идёт внутри одной сессии/выдачи, снижая шум от различий пользователей. При этом важно интерпретировать результат как сигнал о сравнительной полезности ранжирования (proxy), а не как прямой эффект на GMV/Revenue.

#### Вопрос: Почему в оффлайне нельзя просто рандомизировать покупателей внутри магазина, и чем помогают matched pairs или synthetic control?

**Ответ:**
В оффлайн-среде наблюдения внутри одной точки (магазина/ресторана) обычно **зависимы**: общие промо-активности, локальная сезонность, персонал, логистика, конкуренты рядом. Если рандомизировать “внутри магазина”, легко получить перетекание воздействия и систематические смещения, а эффект “задушится” шумом и внешними факторами. Поэтому единицей рандомизации делают магазин/регион/город. **Matched pairs** уменьшают дисперсию: вы заранее подбираете похожие точки по трафику/выручке/площади, и сравнение становится более “честным”. **Synthetic control / causal impact** ещё сильнее учитывает тренды и сезонность, строя контроль как взвешенную комбинацию других точек, что помогает отделить реальный эффект от общего движения рынка.

---

### Метрики: бизнес, продукт, ML, технические

Правильная практика — заранее строить **иерархию метрик**: North Star → драйверы → прокси → guardrails. Это снижает риск “улучшили CTR, но убили прибыль” или “улучшили качество модели, но ухудшили задержки”.

**1) Business (North Star)**

* GMV/Revenue/Margin, Retention, LTV, CAC payback.

* Важны бизнесу, но часто шумные и медленно реагируют.

**2) Product / Proxy (быстрые драйверы)**

* CTR, Add-to-cart rate, Conversion rate, Search success rate, dwell time.

* В RAG/поиске: доля “answered queries”, “zero results rate”, “query reformulation rate”.

**3) ML/IR метрики (оффлайн качество модели)**

* Для ретрива: Recall@K, MRR, nDCG@K, HitRate.

* Для ранкера: nDCG@K, MAP, pairwise accuracy, calibration (если скор интерпретируется как вероятность).

**4) Technical / Guardrails**

* Latency p95/p99, error rate, timeouts, crashes, GPU/CPU, стоимость запроса, размер индекса/векторов.

* Guardrails формулируются как “не хуже X”: например, p99 latency не выше +10%, error rate не выше 0.1 п.п.

**Пример иерархии для поиска/ранжирования**

* North Star: GMV per search session.

* Drivers: orders per session × AOV.

* Proxy: CTR@topK, add-to-cart, query success.

* ML: nDCG@10 (ранкер), Recall@100 (ретрив).

* Guardrails: p99 latency, timeouts, 5xx.

#### Вопрос: Как на практике связать оффлайн-метрики (nDCG/Recall) с онлайн-прокси (CTR/CR), чтобы не попасть в ловушку “оффлайн вырос — онлайн не вырос”?

**Ответ:**
Связка строится через явную гипотезу “какой механизм ведёт от улучшения оффлайна к продукту”: например, рост **Recall@100** у ретрива должен уменьшить “zero results rate” и повысить вероятность релевантного клика; рост **nDCG@10** у ранкера должен увеличить долю кликов/добавлений в корзину на топ-позициях. Дальше важно заранее выбрать **diagnostic метрики**, которые объясняют, где цепочка порвалась: если nDCG вырос, а CTR нет — проверьте latency, изменения постпроцессинга, bias/дрейф данных, “mismatch логов”, или то, что модель улучшила “другие” сегменты, а не ключевые. Иерархия метрик помогает: ML-метрики — как индикатор потенциала, proxy — как быстрый онлайн-сигнал, а business — как конечная проверка ценности.

#### Вопрос: Как правильно формулировать guardrails, чтобы они реально защищали прод и не убивали эксперименты?

**Ответ:**
Guardrails должны быть (1) **измеримыми быстро**, (2) **привязанными к рискам**, и (3) сформулированными как “не хуже X” с понятным порогом, например: p99 latency не выше +10%, error rate не выше 0.1 п.п., timeouts не растут, стоимость запроса не превышает лимит. Практически это значит, что guardrails — не “всё подряд”, а ограниченный набор метрик, которые предотвращают деградацию пользовательского опыта или SRE-инциденты. Если guardrails слишком жёсткие или их слишком много, эксперименты будут “падать” из-за шума, поэтому пороги согласуют с инженерией/продом и часто дополняют стратегией rollout (canary → расширение), чтобы ловить проблемы до 50/50.

---

### Стат для ML, A/B: гипотезы, значимость, мощность

Задача статистики — ответить: “наблюдаемый эффект реален или это шум?”, и “сколько данных нужно, чтобы заметить эффект заданного размера?”.

#### 1) Формулировка гипотез

* **H0**: изменений нет (разница = 0).

* **H1**: изменения есть (двусторонняя) или улучшение (односторонняя — осторожно, заранее фиксировать).

* Ошибки:

  * Type I (α): ложноположительная (обычно 0.05).

  * Type II (β): ложноотрицательная; мощность 1−β1−β обычно 0.8–0.9.

#### Вопрос: Почему одностороннюю H1 нужно фиксировать заранее и чем рискован “переключатель” односторонняя/двусторонняя по результатам?

**Ответ:**
Односторонняя гипотеза (“эффект только в сторону улучшения”) уменьшает порог для значимости по сравнению с двусторонней, поэтому, если выбрать её постфактум, вы фактически повышаете шанс **ложноположительного** результата (Type I). Корректная практика — заранее зафиксировать, что вы готовы считать валидным только улучшение и что ухудшение вас не интересует как “открытие эффекта”. Если же после просмотра данных “переобуться” (вчера — двусторонняя, сегодня — односторонняя, потому что так получилось), это превращается в форму подглядывания и раздувает α, а значит ухудшает доверие к выводам и реплицируемость.

#### Вопрос: Как связаны α, β и мощность (1−β), и почему “мощность 0.8” — это не магическое число, а компромисс?

**Ответ:**
α задаёт вероятность ложноположительного вывода (вы “увидели эффект”, которого нет), β — вероятность ложноотрицательного (эффект есть, но вы его не заметили), а мощность 1−β — вероятность обнаружить эффект заданного размера (например, MDE) при выбранном α. Повышая мощность (снижая β), вы обычно увеличиваете требования к объёму данных/длительности теста. “0.8–0.9” — практичный компромисс между стоимостью эксперимента и риском пропустить важный эффект: для дорогих решений (например, изменения ранжирования с риском выручки) часто выбирают ближе к 0.9, для быстрых итераций — 0.8, но решение должно соответствовать цене ошибки и доступному трафику.

#### 2) Базовые тесты по типу метрики

**A) Доли / конверсии (CR, CTR)**

* Часто используют z-test для двух пропорций или биномиальные модели.

* Пример: конверсия A = 5.0%, B = 5.3% — проверяем значимость разницы.

**B) Средние (time spent, средний чек, latency)**

* t-test при “примерно нормальном” распределении среднего (или больших выборках).

* Но для денег/чека часто тяжёлые хвосты → лучше:

  * лог-трансформация,

  * winsorization,

  * бутстрап разницы средних/медиан.

**C) “Тяжёлые” метрики и ratio-метрики**

* Пример ratio: ARPU = revenue/users, CTR = clicks/impressions.

* Правильнее тестировать через бутстрап или delta method (для дисперсии отношения), иначе легко ошибиться.

#### Вопрос: Почему для денег/чека и latency часто “ломается” t-test и как помогают лог-трансформация или бутстрап?

**Ответ:**
Метрики денег (revenue, средний чек) и иногда latency имеют **тяжёлые хвосты**: редкие большие значения сильно влияют на среднее и дисперсию, из-за чего предположения t-test о “хорошем” поведении распределения могут быть нарушены, а выводы становятся нестабильными. Лог-трансформация делает распределение более “собранным” и снижает влияние экстремальных значений; winsorization ограничивает хвосты, уменьшая чувствительность к выбросам. Бутстрап не требует строгой нормальности и позволяет оценивать распределение разницы (средних/медиан) эмпирически, что часто надёжнее для heavy-tail сценариев.

#### Вопрос: Почему ratio-метрики (ARPU, CTR) нельзя наивно тестировать “как обычные средние” и чем полезен delta method?

**Ответ:**
Ratio-метрика — это отношение двух случайных величин (например, clicks/impressions), и её дисперсия зависит от вариативности числителя и знаменателя и их зависимости. Наивное сравнение средних ratio по “сырым” значениям может дать смещение или неверную оценку дисперсии, особенно если знаменатель сильно колеблется между пользователями/днями. Delta method даёт аппроксимацию дисперсии функции от случайных величин (в данном случае отношения), что помогает корректно построить тест/CI. Бутстрап — альтернативный практичный путь: он напрямую оценивает распределение ratio и разницы, снижая риск “легко ошибиться” на сложных метриках.

#### 3) Доверительные интервалы (CI) вместо “p-value магии”

Практика бигтеха: показывать **эффект + CI**.

* Пример формулировки: “uplift +1.2% (95% CI: +0.2…+2.1), p=0.03”.

* Если CI пересекает 0 — эффект статистически не подтверждён при выбранном уровне.

#### Вопрос: Почему “эффект + 95% CI” полезнее для решения, чем один p-value, особенно для product/business метрик?

**Ответ:**
p-value отвечает на узкий вопрос про совместимость наблюдения с H0, но плохо показывает **масштаб и неопределённость** эффекта. CI сразу даёт диапазон правдоподобных значений uplift: вы видите, насколько эффект может быть “маленьким” или “большим” и пересекает ли он 0. Для продуктовых решений это критично: даже если p-value < 0.05, CI может показывать, что реальный эффект близок к нулю и практически неважен; и наоборот, p-value может быть > 0.05 при недостаточной мощности, но CI уже исключает “катастрофу” и допускает полезный рост. То есть CI напрямую поддерживает обсуждение “это важно бизнесу?” и “какой риск мы берём?”.

#### Вопрос: Что значит “CI пересекает 0” в терминах принятия решения и как это сочетается с guardrails?

**Ответ:**
Если CI для эффекта пересекает 0, то при выбранном уровне доверия нельзя уверенно сказать, что эффект отличен от нуля: данные совместимы и с небольшим минусом, и с небольшим плюсом. В решениях это часто означает “нет доказательства улучшения”, но не обязательно “точно нет эффекта” — возможно, эксперимент недомощный или эффект меньше MDE. Здесь guardrails помогают принять безопасное решение: даже если primary-метрика не подтверждена, вы можете смотреть, не нарушены ли пороги p99 latency/error rate и нет ли признаков деградации. Тогда выбор может быть “не катить, собрать больше данных/улучшить дизайн” вместо рискованного релиза “на удачу”.

#### 4) MDE и расчёт длительности эксперимента

* **MDE (Minimum Detectable Effect)**: минимальный эффект, который хотим уметь обнаруживать.

* Если трафика мало, то тест с MDE=0.5% может быть бессмысленным: он будет длиться слишком долго и всё равно будет нестабилен.

* Практика: фиксировать MDE до запуска и согласовывать с бизнесом: “мы готовы считать успехом +2% к CR, меньше — неважно”.

#### Вопрос: Как выбрать MDE так, чтобы он был и “бизнес-значимым”, и реализуемым по трафику?

**Ответ:**
MDE выбирают на пересечении двух осей. Первая — **практическая значимость**: какой uplift реально меняет решение (например, +2% к CR может быть важен, а +0.2% — нет, если это не окупает риски/стоимость). Вторая — **реализуемость**: при текущем трафике и дисперсии метрики сможете ли вы вообще заметить такой эффект за разумное время. Если вы поставите MDE слишком маленьким при малом трафике, эксперимент станет слишком долгим и всё равно будет нестабилен. Поэтому MDE фиксируют до запуска и согласуют с бизнесом как “порог смысла”, а не как “минимум, который хотелось бы”.

#### Вопрос: Почему тест может быть “нестабилен” при маленьком MDE и ограниченном трафике, даже если формально его можно дождаться?

**Ответ:**
Когда MDE слишком мал, требуемая выборка растёт, и эксперимент становится чувствительным к внешним факторам: сезонность, промо, изменения ассортимента, логирование, краткосрочные тренды. В длинных тестах выше шанс, что “параллельно” изменится среда, и вы измеряете смесь эффектов, а не влияние одной фичи. Кроме того, накопление технических сбоев (например, дрейф данных или изменение latency) повышает риск нарушения предпосылок. Поэтому “дождаться” можно, но качество вывода ухудшается: вы тратите время и всё равно получаете широкие CI и спорную интерпретацию.

#### 5) SRM (Sample Ratio Mismatch)

Проверка, что сплит не сломан: ожидали 50/50, получили 55/45 — значит, могли “потечь” идентификаторы, фильтрация, логирование.

* Типично проверяют тестом согласия (хи-квадрат) и останавливают эксперимент при SRM.

#### Вопрос: Какие практические причины чаще всего приводят к SRM в A/B, кроме “рандомайзер сломался”?

**Ответ:**
SRM часто появляется из-за “неочевидных” инженерных причин: разные правила фильтрации трафика для вариантов, недологированная часть событий, ошибки в присвоении user_id/device_id, кэширование, которое закрепляет пользователя за вариантом не так, как ожидается, или баги в rollout (например, часть пользователей всегда попадает в контроль из-за флага). Ещё один частый источник — “пересечения” и дедупликация: вы считаете уникальных пользователей по одному идентификатору, а сплит делается по другому. Поэтому SRM — не просто статистическая мелочь, а сигнал, что сравнение групп может быть смещено системно.

#### Вопрос: Почему при SRM лучше останавливать эксперимент, а не “ну ладно, поправим весами”?

**Ответ:**
SRM означает, что распределение по группам отличается от ожидаемого неслучайно, и это часто коррелирует с систематическим смещением состава групп (например, “особый” сегмент пользователей чаще попал в вариант B из-за бага). В таком случае любая оценка эффекта становится недоверенной: вы измеряете не влияние изменения, а влияние перекоса аудитории/логирования. “Поправить весами” иногда возможно в исследовательском анализе, но это редко полностью восстанавливает причинность и обычно не снимает вопросы к качеству данных. Для продуктового решения безопаснее остановить, исправить причину и перезапустить, чем принять решение на потенциально смещённых данных.

#### 6) Multiple testing и “подглядывание”

* **A/B/n** и много метрик → растёт шанс ложноположительных → нужны поправки (Bonferroni/Holm/FDR).

* **Peeking** (каждый день смотреть p-value и останавливать, когда стало <0.05) раздувает Type I ошибку; решение: фиксированный горизонт или sequential testing.

#### Вопрос: Почему “много метрик” превращается в проблему ложноположительных, даже если каждая метрика тестируется на α=0.05?

**Ответ:**
α=0.05 означает, что даже при отсутствии эффекта вы ожидаете около 5% ложноположительных срабатываний для одного теста. Если вы проверяете много метрик или много вариантов (A/B/n), вероятность “хоть где-то случайно выстрелило” быстро растёт. В результате команда легко найдёт “значимый” uplift на второстепенной метрике и примет неверное решение. Поправки вроде Bonferroni/Holm/FDR снижают риск ложноположительных на множестве проверок и дисциплинируют вывод: вы либо заранее фиксируете primary и вторичные метрики, либо корректно учитываете множественность.

#### Вопрос: Что именно плохого в ежедневном peeking и почему фиксированный горизонт или sequential testing решают проблему?

**Ответ:**
При peeking вы многократно “спрашиваете данные”, не меняя α, и тем самым увеличиваете общий шанс поймать случайное p-value < 0.05 даже при нулевом эффекте. Это скрытая форма multiple testing по времени. Фиксированный горизонт решает это дисциплиной: вы смотрите результат один раз в конце, сохраняя контроль α. Sequential testing (если используется корректно) меняет правила остановки и пороги так, чтобы вероятность ложноположительного оставалась контролируемой при многократных просмотрах, позволяя при этом завершать эксперимент раньше, когда сигнал достаточно сильный.

#### 7) Variance reduction: CUPED

Если есть “предэкспериментальная” метрика (например, revenue за прошлую неделю), её можно использовать как ковариату и снизить дисперсию эффекта → быстрее поймать сигнал.

* Смысл: сравниваем не сырые значения, а значения “скорректированные” по базовому уровню.

#### Вопрос: В каких случаях CUPED даёт наибольший выигрыш и что нужно проверить перед применением?

**Ответ:**
CUPED наиболее полезен, когда предэкспериментальная метрика (например, прошлый revenue пользователя) **сильно коррелирует** с метрикой в эксперименте: тогда “базовый уровень” объясняет часть вариативности, и дисперсия эффекта падает. Перед применением важно убедиться, что ковариата измерена корректно и одинаково для обеих групп, и что она действительно относится к “до” периоду (не затронута экспериментом). Также важно следить за корректной агрегацией (часто по пользователю) и за тем, чтобы корректировка не смешивала пользователей с разным временем активности (например, если окно “до” для части пользователей неполное).

#### Вопрос: Как CUPED сочетается с SRM и множественными проверками: он “чинит” проблемы дизайна?

**Ответ:**
CUPED — это способ снизить дисперсию, но он не исправляет фундаментальные проблемы дизайна. Если у вас SRM или перекос состава групп, CUPED не вернёт причинность: вы просто более уверенно оцените смещённый эффект. С множественными проверками CUPED тоже не “отменяет” необходимость поправок: он ускоряет обнаружение сигнала, но не меняет того факта, что много тестов увеличивает риск ложноположительных. Поэтому порядок здравого смысла такой: (1) проверить корректность сплита и данных (включая SRM), (2) зафиксировать метрики/гипотезы, (3) применять variance reduction (CUPED) как ускоритель, а не как костыль.

#### Вопрос: Зачем в A/B для ML-инженера одновременно думать про значимость, мощность и MDE, а не только про “p<0.05”?

**Ответ:**
Потому что “p<0.05” без контекста не отвечает на вопросы, важные для прод-решения: заметили ли вы **достаточно большой** эффект (MDE/практическая значимость), была ли у теста **способность** обнаружить нужный размер эффекта (мощность), и насколько вы рискуете принять случайный результат (значимость/α). В ML-проектах часто есть компромиссы “качество vs latency/стоимость”, поэтому решение должно быть устойчивым: вы заранее определяете, какой uplift стоит риска, сколько времени можно тратить на эксперимент, и какие guardrails нельзя нарушать. Это делает выводы воспроизводимыми и защищает от “p-value магии”.

#### Вопрос: Как связать статистическую часть A/B с инженерными артефактами (runbook, monitoring, release artifacts), чтобы результат был реально раскатываемым?

**Ответ:**
Статистика даёт “есть ли эффект и какой диапазон”, а инженерные артефакты обеспечивают, что этот эффект **не превратится в инцидент** и будет воспроизводим. На практике это выглядит так: ещё до запуска вы фиксируете дизайн (unit of randomization, MDE, метрики), добавляете guardrails в мониторинг, прописываете план отката и canary rollout в release artifacts, и готовите runbook на случай роста latency/error rate или деградации качества. После эксперимента вы не просто говорите “значимо”, а показываете effect+CI, проверку SRM, соблюдение guardrails и готовность к откату — это и есть мост от статистики к эксплуатации.

---

### Сквозной пример (поиск/ранжирование) — от гипотезы до решения

**Ситуация:** меняем reranker в поиске (BERT → более лёгкий distilled, но с новым обучением).

1. **Гипотеза**

* H1H1: новый reranker увеличит nDCG@10 оффлайн и даст рост Search CTR@10 онлайн без ухудшения p99 latency.

2. **Выбор метрик**

* Primary: CTR@10 (proxy), Search conversion.

* Secondary: GMV/session (если хватает мощности).

* Guardrails: p99 latency, timeouts, error rate.

* Diagnostic: nDCG@10 оффлайн, Recall@100 ретривера (чтобы не маскировать проблему retrieval-стадии).

3. **Дизайн**

* Online A/B 50/50, unit=user_id, срок по power analysis.

* SRM check ежедневно.

* Ограничить rollout: сначала 5% трафика (canary), затем 50%.

4. **Статистика**

* CTR: тест пропорций (или бутстрап по пользователю).

* Revenue: бутстрап (из-за heavy tail) + отчёт CI.

* Несколько метрик → контролируем множественные проверки.

5. **Интерпретация**

* Если CTR вырос, но GMV не изменился: возможно, клики стали “пустыми” → анализ downstream (add-to-cart, purchase).

* Если оффлайн nDCG вырос, а онлайн CTR не вырос: вероятны bias/дрейф, mismatch логов, проблемы латентности или UI.

#### Вопрос: Почему в этом примере важно иметь одновременно diagnostic метрики (nDCG/Recall) и guardrails (p99/timeouts), а не только primary CTR@10?

**Ответ:**
CTR@10 показывает быстрый продуктовый сигнал, но не объясняет причину изменений. Diagnostic метрики (nDCG@10, Recall@100) позволяют понять, **где** улучшение/ухудшение происходит: в ранкере, в ретривере или в данных. Guardrails защищают прод: даже если CTR растёт, рост p99 latency или timeouts может ухудшить общий опыт и привести к инцидентам/потере конверсии на длинной дистанции. Комбинация нужна, чтобы решение было “инженерно корректным”: вы подтверждаете эффект, понимаете механизм и не нарушаете эксплуатационные ограничения.

#### Вопрос: Как интерпретировать ситуацию “оффлайн nDCG вырос, а онлайн CTR не вырос” так, чтобы это привело к конкретным действиям, а не к спору “оффлайн не работает”?

**Ответ:**
Эту ситуацию полезно разложить на проверяемые гипотезы по пайплайну. (1) Проверить, не ухудшилась ли latency и не “съела” ли она эффект (guardrails, p95/p99, timeouts). (2) Проверить mismatch логов и разметки: совпадает ли то, что считали оффлайн, с тем, что реально показывается пользователю после постпроцессинга и бизнес-правил. (3) Посмотреть сегменты: возможно, nDCG вырос на неключевых запросах, а на head-запросах не изменился или упал. (4) Убедиться, что retrieval не стал хуже (Recall@100), иначе ранкеру “нечего ранжировать”. Такой план превращает расхождение оффлайн/онлайн в последовательный дебаг, а не в дискуссию на уровне веры в метрики.
