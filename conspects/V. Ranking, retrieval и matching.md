Полное руководство по построению поисковых систем, RAG-пайплайнов и рекомендательных движков. От классических алгоритмов до SOTA трансформерных архитектур 2025 года.

---

## Эволюция поисковых систем

Современный поиск — это не просто "найти документ по ключевым словам". Это сложная многоступенчатая система, где каждый этап оптимизирован под свою задачу. Если в 2000-х годах доминировал TF-IDF, то сегодня стандартом стала гибридная архитектура: Sparse (BM25) + Dense (Neural Embeddings) + Reranking (Cross-Encoders).

**Основная проблема:** Как из 100 миллионов документов за 50 миллисекунд выдать топ-10, который реально решит задачу пользователя? Ответ: двухстадийная архитектура.

---



#### Вопрос: Почему современный поиск почти всегда строят как многоступенчатую систему, а не как один «умный» ранкер?
**Ответ:**
Потому что у поиска есть жесткие ограничения по времени и масштабу: из миллионов документов нужно за миллисекунды собрать кандидатов и только потом «дорого» довести порядок. Stage 1 оптимизируют под скорость и Recall@K, Stage 2 — под качество топа (NDCG/MRR/Precision), поэтому вместе получается и быстро, и качественно.
#### Вопрос: Зачем в современных пайплайнах сочетают Sparse (BM25) и Dense (эмбеддинги), если Dense «семантичнее»?
**Ответ:**
Sparse лучше ловит точные совпадения (артикулы, имена, редкие термины) и предсказуем в фильтрах, а Dense хорошо работает на перефразах и синонимах. Их комбинация повышает устойчивость качества на разных типах запросов и уменьшает риск «пропуска» релевантных документов на этапе retrieval.

## 1. ФУНДАМЕНТАЛЬНЫЕ КОНЦЕПЦИИ

### 1.1. Information Retrieval Models (Теория поиска)

Исторически существовало три подхода к моделированию релевантности:

#### A. Boolean Retrieval (Булева модель)

Самая ранняя модель. Документы представлены как множества термов. Запрос — это булево выражение.

- _Запрос:_ `(Python AND "machine learning") OR TensorFlow`
    
- _Результат:_ Все документы, удовлетворяющие логике (без ранжирования).
    
- **Используется:** Elasticsearch, правовые базы данных (точный поиск по статьям).
    
- _Минус:_ Нет понятия "более релевантный". Либо документ подходит (1), либо нет (0).
    



#### Вопрос: Где булева модель полезнее всего и почему она до сих пор используется в системах типа Elasticsearch?
**Ответ:**
Она дает строгий контроль: must/should/must_not, фильтры по полям, точные фразы. Это важно в сценариях, где условия должны выполняться гарантированно (юридические базы, поиск по атрибутам). В продакшене булевы условия часто используют как фильтрацию, а ранжирование делают отдельным скорером.
#### Вопрос: Как компенсировать отсутствие ранжирования в чистой булевой модели?
**Ответ:**
Делают «двухслойный» подход: сначала булевый отбор кандидатов по must‑условиям, затем ранжирование BM25/LTR/нейросетью. Либо переводят часть условий в soft‑constraints (should) с весами, чтобы получить ранжированный список при сохранении строгих фильтров.

#### B. Vector Space Model (Векторная модель)

Документы и запросы — это векторы в пространстве термов.

- Каждый терм (слово) — это измерение.
    
- Вес терма в документе: TF-IDF.
    
- **Релевантность:** Косинусное расстояние между $\vec{q}$ и $\vec{d}$.  
    sim(q,d)=q⃗⋅d⃗∣∣q⃗∣∣⋅∣∣d⃗∣∣sim(q,d)=∣∣q∣∣⋅∣∣d∣∣q⋅d
    



#### Вопрос: Почему косинусная близость является стандартом для VSM с TF‑IDF?
**Ответ:**
Косинус сравнивает направление векторов и снижает влияние длины документа (нормы), что важно для текстов разной длины. Это делает ранжирование более устойчивым: документ не выигрывает только потому, что он длиннее и содержит больше слов.
#### Вопрос: В чем ограничение TF‑IDF/VSM по сравнению с BM25?
**Ответ:**
TF‑IDF часто линейно усиливает влияние TF, поэтому «спам» повторениями может переоцениваться, и нормализация длины менее гибкая. BM25 добавляет насыщение TF (k1) и управляемую нормализацию длины (b), что обычно дает более стабильный скор в продакшене.

#### C. Probabilistic Model (BM25)

Вопрос: "Какова вероятность, что документ $d$ релевантен запросу $q$?"

- BM25 — это функция ранжирования, основанная на вероятностной модели Robertson & Sparck Jones.
    
- Учитывает насыщение частоты терма (если слово встречается 100 раз, это не в 100 раз важнее, чем 1 раз).
    

---



#### Вопрос: Что означает насыщение TF в BM25 и почему это важно?
**Ответ:**
Насыщение означает убывающую отдачу: повторение терма 20 раз не делает документ в 20 раз релевантнее, чем 1 раз. Это защищает от терм-спама и делает скор более «человеческим» для реального ранжирования.
#### Вопрос: Как параметр b влияет на нормализацию длины документа и как понять, что он «слишком большой»?
**Ответ:**
b задает, насколько сильно длина документа влияет на скор: b=0 игнорирует длину, b=1 нормализует максимально. Если b слишком большой, длинные, но полезные документы начинают системно опускаться; это видно по деградации NDCG на запросах, где ответы часто в длинных статьях/мануалах.



#### Вопрос: Как отличается понятие релевантности в булевой, векторной и вероятностной моделях?
**Ответ:**
В булевой модели релевантность бинарная (да/нет), в векторной — непрерывная (косинусная близость запроса и документа в TF‑IDF‑пространстве), в вероятностной — скор как оценка релевантности с учетом редкости термов и длины документа (BM25). Это отражает эволюцию от фильтрации к полноценному ранжированию.
#### Вопрос: Почему переход от «matching» к «ranking» считается ключевым для пользовательского поиска?
**Ответ:**
Потому что пользователю нужен не просто набор подходящих документов, а лучшие вверху. Ранжирование позволяет учитывать важность термов (IDF), частоту (TF), насыщение и позиционную полезность, а затем дополнить это нейросетевыми моделями и поведенческими сигналами.

### 1.2. Two-Stage Retrieval Architecture

Это золотой стандарт для масштабируемых систем (Google, Yandex, OpenAI RAG).[jina+1](https://jina.ai/ru/news/maximizing-search-relevancy-and-rag-accuracy-with-jina-reranker/)​

**Stage 1: Retrieval (Candidate Generation)**

- **Цель:** Быстро отфильтровать 99.99% неподходящих документов.
    
- **Вход:** Запрос $q$, корпус $D$ (10M-1B документов).
    
- **Выход:** Топ-1000 кандидатов.
    
- **Методы:** BM25, Bi-Encoders (Sentence Transformers), ANN Search (HNSW).
    
- **Метрика успеха:** Recall@1000. Если среди 1000 кандидатов есть все релевантные документы, то Recall = 100%.
    

**Stage 2: Ranking (Re-ranking)**

- **Цель:** Отсортировать 1000 так, чтобы топ-10 были идеальными.
    
- **Методы:** Cross-Encoders, LambdaMART, LLM-as-Ranker.
    
- **Метрика успеха:** NDCG@10, Precision@10.
    

**Почему нельзя использовать Cross-Encoder сразу?**  
Cross-Encoder требует Forward Pass для каждой пары (query, doc). Для 10M документов это 10M вызовов BERT (несколько часов). Bi-Encoder можно применить за 1 секунду, потому что эмбеддинги документов предпосчитаны.

---



#### Вопрос: Почему Stage 1 измеряют Recall@K, а Stage 2 — NDCG/MRR/Precision@K?
**Ответ:**
Stage 1 должен покрыть релевантные документы среди кандидатов (важно «не потерять»), поэтому считают Recall@K. Stage 2 работает на небольшом наборе и отвечает за качество верхушки выдачи, поэтому важны позиционные метрики (NDCG@10, MRR) и precision.
#### Вопрос: Как bi‑encoder + ANN делает двухстадийную архитектуру практичной?
**Ответ:**
Bi‑encoder позволяет хранить эмбеддинги документов и искать ближайшие через ANN (HNSW/IVF) быстро. Это дает дешёвый retrieval кандидатов, после чего можно применить дорогой cross‑encoder/ColBERT/LLM для rerank только топ‑K.



#### Вопрос: Какая общая идея объединяет Boolean Retrieval, Vector Space Model и BM25?
**Ответ:**
Все три модели пытаются формализовать релевантность документа запросу: булева — через логические условия, векторная — через близость TF‑IDF-векторов, вероятностная (BM25) — через скор с IDF, насыщением TF и нормализацией по длине. В реальных системах эти подходы часто комбинируют: фильтры/логика + лексический скор + нейросетевые сигналы.
#### Вопрос: Почему даже «супер-модель» не отменяет необходимость архитектуры retrieval → rerank?
**Ответ:**
Потому что точные модели обычно дорогие: cross‑encoder или LLM не могут быть применены ко всему корпусу. Архитектура Stage 1/Stage 2 позволяет сначала быстро сократить пространство (ANN/BM25), а затем применить точный ранкер к небольшому топ‑K, сохраняя и качество, и latency.

## 2. RETRIEVAL METHODS (ПОИСК КАНДИДАТОВ)

### 2.1. Sparse Retrieval (Lexical Search)

#### TF-IDF (Term Frequency - Inverse Document Frequency)

Классическая формула взвешивания термов.  
w(t,d)=tf(t,d)⋅idf(t)w(t,d)=tf(t,d)⋅idf(t)  
Где:

- $tf(t, d)$ — частота терма $t$ в документе $d$ (normalized).
    
- $idf(t) = \log \frac{N}{df(t)}$ — обратная частота документа ($N$ — всего документов, $df(t)$ — в скольких встречается терм $t$).
    

**Интуиция:** Слово "the" встречается везде → низкий IDF → неважно. Слово "трансформер" встречается редко → высокий IDF → важно для поиска.



#### Вопрос: Почему IDF делает TF‑IDF сильнее простого подсчета совпадений слов?
**Ответ:**
IDF повышает вес редких и информативных термов и снижает вес частых. Благодаря этому документы с ключевыми, «уточняющими» словами запроса поднимаются выше, даже если общий объем текста небольшой.
#### Вопрос: Зачем в TF‑IDF применяют лог‑скейлинг или нормализацию TF?
**Ответ:**
Чтобы уменьшить перекос в пользу длинных документов и сгладить влияние очень частых повторений терма. Это делает ранжирование более устойчивым и ближе по поведению к идеям насыщения TF, которые формализованы в BM25.

#### BM25 (Best Matching 25) [SOTA Sparse]

Улучшение TF-IDF с **saturation функцией**.  
score(q,d)=∑t∈qIDF(t)⋅tf(t,d)⋅(k1+1)tf(t,d)+k1⋅(1−b+b⋅∣d∣avgdl)score(q,d)=∑t∈qIDF(t)⋅tf(t,d)+k1⋅(1−b+b⋅avgdl∣d∣)tf(t,d)⋅(k1+1)  
Параметры:

- $k_1 \in [1.2, 2.0]$: Контролирует насыщение TF. Если $k_1 = 0$, TF игнорируется.
    
- $b \in $: Нормализация по длине документа. $b=1$ — полная нормализация, $b=0$ — игнорируем длину.
    
- $avgdl$: Средняя длина документа в корпусе.
    

**Когда BM25 лучше Dense:**[deepschool](https://blog.deepschool.ru/llm/rag-ot-pervoj-versii-k-rabochemu-resheniyu/)​

- Запросы с точными совпадениями (артикулы, коды, имена).
    
- Короткие документы (твиты, заголовки).
    
- Домены с большим количеством уникальных терминов (медицина, юриспруденция).
    

---



#### Вопрос: Почему BM25 обычно заменяет TF‑IDF как базовый sparse ранкер?
**Ответ:**
BM25 улучшает TF‑IDF за счет насыщения TF и нормализации по длине (b), сохраняя сильный IDF‑сигнал. В результате он устойчивее к спаму, лучше работает на документах разной длины и чаще дает прирост NDCG/Precision в реальных корпусах.
#### Вопрос: Как подбирать k1 и b на практике?
**Ответ:**
Их тюнят на валидации по NDCG/Recall, начиная с разумных дефолтов. Дальше смотрят, не страдают ли длинные документы (b) и не переоценивается ли повторяемость термов (k1), и подбирают компромисс под конкретный корпус.



#### Вопрос: Когда sparse retrieval обычно лучше dense retrieval?
**Ответ:**
Когда важны точные совпадения: артикулы, модели, версии, редкие доменные термины, имена. В этих случаях BM25 быстро и стабильно поднимает нужные документы без риска семантического «дрейфа».
#### Вопрос: Как фильтры и поля документа усиливают sparse retrieval в продакшене?
**Ответ:**
Фильтры по метаданным (категория, язык, дата, бренд) сокращают пространство поиска и повышают precision. Дополнительно можно по-разному взвешивать поля (title > body), чтобы BM25/TF‑IDF отражали важность совпадений в ключевых местах.

### 2.2. Dense Retrieval (Semantic Search)

#### Bi-Encoders (Two-Tower Architecture)

Энкодер запроса и энкодер документа работают **независимо**.

1. Запрос $q$ → $\vec{v}_q = Encoder_Q(q)$ (размерность 768 или 384).
    
2. Документ $d$ → $\vec{v}_d = Encoder_D(d)$ (предпосчитан заранее).
    
3. Релевантность: $score(q, d) = CosSim(\vec{v}_q, \vec{v}_d)$.
    

**Почему это быстро?**  
Мы храним эмбеддинги всех документов в векторной базе данных (Qdrant, Milvus). Поиск сводится к ANN (Approximate Nearest Neighbors) — одной операции.

**Топ модели (2024-2025):**

- **Sentence-BERT (SBERT):** Пионер. Обучен на NLI + STS датасетах.
    
- **E5 (Microsoft):** Prefix-based. Запрос начинается с `"query: "`, документ с `"passage: "`.
    
- **BGE (BAAI):** Китайская модель, SOTA на MTEB бенчмарке.
    
- **Voyage AI:** Проприетарная модель (через API), лучшая по качеству.
    

**Обучение Bi-Encoder:**

- **Contrastive Loss (InfoNCE):**  
    L=−log⁡exp⁡(sim(q,d+)/τ)exp⁡(sim(q,d+)/τ)+∑d−exp⁡(sim(q,d−)/τ)L=−logexp(sim(q,d+)/τ)+∑d−exp(sim(q,d−)/τ)exp(sim(q,d+)/τ)  
    Где $d^+$ — позитивный документ, $d^-$ — негативные (hard negatives критичны для качества).
    

---



#### Вопрос: Почему bi‑encoder — стандарт для retrieval в RAG и поиске?
**Ответ:**
Потому что запрос и документ кодируются независимо: эмбеддинги документов можно предпосчитать и хранить, а поиск сводится к ANN по векторам. Это дает масштабируемость на миллионы/десятки миллионов документов.
#### Вопрос: Как contrastive learning с in-batch negatives ускоряет обучение bi‑encoder?
**Ответ:**
В батче каждый позитив служит негативом для других запросов, что дает много отрицательных примеров «почти бесплатно». Чтобы модель не училась на слишком легких негативов, дополнительно добавляют hard negatives из BM25/ANN.

#### Approximate Nearest Neighbors (ANN)

Как найти ближайших соседей среди 100 миллионов векторов?

**1. HNSW (Hierarchical Navigable Small World)** [Стандарт индустрии]

- Строит **иерархический граф**. На верхнем уровне — мало узлов, связи между далекими регионами. На нижнем — все точки, детальные связи.
    
- **Поиск:** Начинаем с верхнего слоя, "прыгаем" к ближайшему узлу, спускаемся на уровень ниже, повторяем.
    
- **Параметры:**
    
    - `ef_construction`: Размер динамического списка кандидатов при построении (больше = точнее, но медленнее строится).
        
    - `M`: Число связей каждого узла (обычно 16-64).
        
- **Используется:** Qdrant, Milvus, Weaviate.
    

**2. IVF (Inverted File Index)**

- Разбиваем пространство на $N$ ячеек (Voronoi cells) с помощью K-Means.
    
- Каждый вектор попадает в одну ячейку.
    
- **Поиск:** Ищем только в $k$ ближайших ячейках (обычно $k=10-50$).
    
- **Product Quantization (PQ):** Дополнительная компрессия. Вектор 768-dim сжимается до 64 байт.
    
- **Используется:** Faiss (Facebook AI Similarity Search).
    

**3. ScaNN (Google)**

- Комбинация ANN + квантизация + re-scoring.
    
- Сначала ищем грубо (квантованные векторы), затем уточняем (full-precision для топ-100).
    

---



#### Вопрос: Почему ANN необходим для dense retrieval на больших индексах?
**Ответ:**
Точный поиск ближайших соседей требует сравнить запрос с каждым вектором, что слишком медленно. ANN использует структуры (например, графы HNSW или IVF‑кластеризацию), чтобы находить почти те же топ‑результаты за миллисекунды.
#### Вопрос: Какие параметры ANN обычно тюнят и как это связано с SLO?
**Ответ:**
В HNSW тюнят `ef` и `M`, в IVF — `nprobe`, в PQ — степень квантизации. Увеличение параметров повышает Recall@K, но растит latency/память, поэтому их подбирают под целевой p95/p99 и нагрузку.



#### Вопрос: Почему dense retrieval на эмбеддингах полезен именно для «семантики»?
**Ответ:**
Эмбеддинги группируют синонимы и перефразы рядом, поэтому система может найти релевантное без точного совпадения слов. Это особенно полезно в вопросно‑ответных сценариях и при естественно-языковых запросах.
#### Вопрос: Какие типичные ошибки dense retrieval и как их уменьшить?
**Ответ:**
Частые ошибки: промахи на идентификаторах/цифрах и «семантический дрейф» к похожей теме. Уменьшают их гибридом со sparse, фильтрами по метаданным и обучением bi‑encoder с hard negatives, близкими к реальным confusers.

### 2.3. Hybrid Search (Sparse + Dense)[jina](https://jina.ai/ru/news/maximizing-search-relevancy-and-rag-accuracy-with-jina-reranker/)​

**Проблема:** BM25 хорош для keyword matching, Dense — для семантики. Как их объединить?

#### Reciprocal Rank Fusion (RRF)

Простой и эффективный метод.  
RRF(d)=∑r∈{BM25,Dense}1k+rankr(d)RRF(d)=∑r∈{BM25,Dense}k+rankr(d)1  
Где $k=60$ (константа из статьи авторов), $rank_r(d)$ — позиция документа в списке метода $r$.

**Пример:**

- BM25: [doc1, doc5, doc3] → $rank_{BM25}(doc1) = 1$
    
- Dense: [doc3, doc1, doc7] → $rank_{Dense}(doc1) = 2$
    
- $RRF(doc1) = \frac{1}{60+1} + \frac{1}{60+2} \approx 0.032$
    

**Преимущества:**

- Не нужна нормализация скоров (работает только с рангами).
    
- Устойчив к выбросам (один метод ошибся — второй компенсирует).
    



#### Вопрос: Почему RRF удобен для гибридного поиска без сложной нормализации?
**Ответ:**
RRF использует позиции (ранги), а не абсолютные скоры, поэтому не нужно приводить BM25 и cosine similarity к одной шкале. Это делает метод простым и устойчивым к изменениям распределений скоров.
#### Вопрос: Как влияет параметр k в RRF на итоговый ранжированный список?
**Ответ:**
k сглаживает вклад позиций: меньший k усиливает влияние самых верхних мест, больший — делает вклад более равномерным по списку. k подбирают экспериментально по NDCG/Recall, ориентируясь на целевой top‑K.

#### Learned Fusion (LTR Approach)

Обучаем XGBoost/CatBoost с фичами:

- `bm25_score`
    
- `dense_score`
    
- `doc_length`
    
- `exact_match_count` (сколько слов запроса есть в документе)
    
- `query_doc_cosine`
    

Модель учится оптимальным весам для каждой фичи.

---



#### Вопрос: Какие фичи обычно используют для learned fusion помимо `bm25_score` и `dense_score`?
**Ответ:**
Часто добавляют признаки exact match, длину документа, количество совпавших токенов, поля (title/body), популярность, свежесть и т.п. Это помогает модели понять, когда лексический сигнал важнее, а когда — семантический.
#### Вопрос: Какие риски у learned fusion по кликам и как их снизить?
**Ответ:**
Клики смещены позицией (position bias) и презентацией сниппета. Снижают риск логированием экспозиций, counterfactual‑коррекцией, аккуратным построением тренировочного датасета и валидацией на независимой разметке/оффлайн метриках.



#### Вопрос: Почему hybrid поиск часто дает лучший recall, чем использование только одного подхода?
**Ответ:**
Sparse и dense ошибаются по‑разному: sparse пропускает перефразы, dense хуже на точных совпадениях и редких термах. Слияние кандидатов повышает вероятность, что нужный документ попадет в общий топ‑K на Stage 1.
#### Вопрос: На каком этапе лучше делать гибрид: слияние списков или learned fusion?
**Ответ:**
Если болит recall — лучше сливать списки уже на retrieval (например, RRF). Если кандидаты уже хорошие, learned fusion/LTR на stage 2 позволяет точнее упорядочить документы, используя `bm25_score` и `dense_score` как фичи.



#### Вопрос: Какие два основных класса retrieval выделяют и чем они принципиально отличаются?
**Ответ:**
Sparse retrieval (TF‑IDF/BM25) опирается на совпадение токенов и IDF, dense retrieval — на семантическую близость эмбеддингов (bi‑encoder + ANN). Sparse выигрывает на точных идентификаторах, dense — на перефразах и синонимах.
#### Вопрос: Почему retrieval должен быть «дешевым», даже если у вас сильный reranker?
**Ответ:**
Потому что reranker работает только на топ‑K. Если retrieval не нашел нужные документы, Stage 2 уже ничего не исправит. Поэтому retrieval оптимизируют под высокий recall при заданной latency и бюджете.

## 3. RANKING & RE-RANKING (ДОВОДКА ПОРЯДКА)

### 3.1. Neural Ranking Architectures

#### A. Cross-Encoders (BERT Reranker)[jina](https://jina.ai/ru/news/maximizing-search-relevancy-and-rag-accuracy-with-jina-reranker/)​

**Идея:** Подаем пару (query, document) как **одну последовательность** в BERT.

```text
[CLS] query tokens [SEP] document tokens [SEP]
```

Self-Attention видит взаимодействия между всеми словами запроса и документа (в отличие от Bi-Encoder, где они кодируются раздельно).

**Архитектура:**

- BERT → [CLS] token → Linear(1) → $score \in \mathbb{R}$
    

**Плюсы:**

- Самая высокая точность (state-of-the-art на MS MARCO).
    

**Минусы:**

- Медленно: нужен Forward Pass для каждой пары (query, doc).
    
- Нельзя предпосчитать: эмбеддинг зависит от обоих входов одновременно.
    

**Production Pipeline:**

1. Bi-Encoder: 10M docs → 1000 кандидатов (1 сек).
    
2. Cross-Encoder: 1000 кандидатов → топ-10 (0.5 сек).
    



#### Вопрос: Почему cross‑encoder обычно самый точный reranker?
**Ответ:**
Потому что query и document проходят через один трансформер, и self‑attention моделирует токен‑к‑токену взаимодействия. Это позволяет учитывать контекстные совпадения, отрицания и уточнения, которые теряются при независимом кодировании.
#### Вопрос: Как уменьшить стоимость cross‑encoder в продакшене?
**Ответ:**
Сокращают K, применяют дистилляцию в меньшую модель, используют ONNX/квантизацию, батчинг и кэширование популярных запросов. Также помогают более дешевые архитектуры типа ColBERT, когда нужен компромисс скорости и качества.

#### B. ColBERT (Late Interaction) [SOTA Balance]

"Золотая середина" между Bi и Cross.

**Механизм:**

1. Запрос $q$ → Набор векторов токенов $E_q = {e_1^q, ..., e_n^q}$ (каждый токен → 128-dim вектор).
    
2. Документ $d$ → $E_d = {e_1^d, ..., e_m^d}$ (предпосчитан).
    
3. **MaxSim:** Для каждого токена запроса находим максимальную схожесть с любым токеном документа.  
    score(q,d)=∑i=1∣q∣max⁡j=1∣d∣CosSim(eiq,ejd)score(q,d)=∑i=1∣q∣maxj=1∣d∣CosSim(eiq,ejd)
    

**Интуиция:** Каждое слово запроса должно "найти себя" в документе. Если слово "Python" в запросе сильно схоже с "Python" в документе, это добавляет в скор.

**Преимущества:**

- Можно предпосчитать $E_d$ (как в Bi-Encoder).
    
- Точность выше, чем у Bi-Encoder (близка к Cross-Encoder).
    
- Компактное хранилище (каждый токен → 128 dim вместо full 768-dim вектора документа).
    

---



#### Вопрос: Что означает late interaction в ColBERT и почему это дает баланс качества и скорости?
**Ответ:**
Документ кодируется в набор токен‑векторов заранее, запрос — онлайн, а взаимодействие происходит на скоринге через MaxSim. Это сохраняет более детальный сигнал, чем один вектор bi‑encoder, но дешевле, чем прогонять cross‑encoder для каждой пары.
#### Вопрос: Какие типы запросов особенно выигрывают от ColBERT?
**Ответ:**
Запросы, где важны отдельные сущности/термы и их соответствие в документе (бренды, названия, технические термины), но при этом возможны перефразы. MaxSim позволяет «подцепить» нужные токены, сохранив семантическую гибкость.



#### Вопрос: Чем cross‑encoder и ColBERT отличаются по вычислительной стоимости?
**Ответ:**
Cross‑encoder обрабатывает пару (query, doc) совместно и требует отдельного прогона для каждой пары, что дорого. ColBERT частично предвычисляет представления документов и делает late interaction (MaxSim), поэтому обычно быстрее и лучше масштабируется.
#### Вопрос: Почему нейросетевые ранкеры часто дают прирост по сравнению с LTR на ручных фичах?
**Ответ:**
Они напрямую моделируют взаимодействие токенов и контекст, улавливают перефразы и сложные соответствия, которые трудно выразить руками. При этом в продакшене их часто комбинируют: нейросетевой скор используют как одну из фич LTR.

### 3.2. Learning to Rank (LTR)

Классический ML подход: обучаем модель (XGBoost, CatBoost, LightGBM) предсказывать релевантность.

#### Типы лоссов:

**1. Pointwise (Независимое предсказание)**

- Задача: Для каждого документа предсказать скор релевантности (0-5) или бинарную метку (0/1).
    
- Лосс: MSE или Binary Cross-Entropy.
    
- _Минус:_ Не учитывает порядок документов в списке. Если мы ошибочно дали релевантному документу скор 0.4, а нерелевантному 0.6, и оба оказались на правильных местах, модель думает, что всё ОК.
    

**2. Pairwise (Сравнение пар)**

- Задача: Для пары $(d_i, d_j)$ предсказать, что $d_i$ более релевантен, чем $d_j$.
    
- **RankNet:** Лосс на вероятность правильного порядка.  
    L=−log⁡σ(si−sj)L=−logσ(si−sj)  
    Где $s_i, s_j$ — скоры документов.
    
- **LambdaMART:** Улучшение RankNet. Градиенты взвешиваются на изменение NDCG при swap'е пары документов.
    

**3. Listwise (Оптимизация всего списка)**

- Задача: Оптимизировать метрику (NDCG, MAP) напрямую.
    
- **ListNet:** Cross-Entropy между истинным распределением позиций и предсказанным.
    
- **ApproxNDCG:** Дифференцируемая аппроксимация NDCG (можно считать градиенты).
    

**Фичи для LTR:**

- Query-dependent: Длина запроса, число стоп-слов.
    
- Document-dependent: Длина документа, PageRank, читабельность (Flesch-Kincaid).
    
- Query-Document: BM25 score, Cosine similarity, Exact match ratio, Jaccard similarity.
    

---



#### Вопрос: Почему pairwise‑лоссы (RankNet/LambdaRank) часто лучше отражают задачу ранжирования, чем pointwise?
**Ответ:**
Потому что ранжирование — относительная задача: важно, чтобы релевантный документ был выше нерелевантного. Pairwise‑лосс напрямую оптимизирует правильные предпочтения, тогда как pointwise может не «наказывать» за неправильные перестановки внутри топа.
#### Вопрос: Что делает LambdaMART эффективным для NDCG?
**Ответ:**
LambdaMART взвешивает «лямбды» градиента по тому, насколько swap пары документов изменит NDCG. Это фокусирует обучение на ошибках, которые сильнее всего влияют на верхнюю часть выдачи.



#### Вопрос: Почему LTR остаётся актуальным рядом с нейросетевыми ранкерами?
**Ответ:**
LTR позволяет гибко смешивать десятки сигналов (BM25, dense_score, бизнес‑фичи, поведенческие данные) и оптимизировать метрику ранжирования на доменных данных. Часто бустинг‑модели быстрее и проще деплоить и мониторить.
#### Вопрос: Какой тип постановки (pointwise/pairwise/listwise) выбирать в LTR?
**Ответ:**
Pointwise проще, но хуже отражает порядок. Pairwise прямо учит предпочтения и часто хорошо работает для ранжирования. Listwise ближе к оптимизации NDCG, но требует больше данных и аккуратности; его обычно пробуют после стабильного pairwise/LambdaMART baseline.

### 3.3. LLM-as-a-Reranker

**Идея:** Использовать GPT-4 / Claude 3.5 / Llama 3 для оценки релевантности.

**Промпт:**

```text
Query: "Как обучить нейросеть на PyTorch?" Document 1: [text...] Document 2: [text...] Rank these documents by relevance to the query. Output: [1, 2] or [2, 1].
```

**Результаты:**

- GPT-4 показывает качество выше Cross-Encoder на сложных запросах (reasoning, multi-hop).
    
- Понимает нюансы (сарказм, контекст, доменные знания).
    

**Проблемы:**

- **Дорого:** $0.01 за 1k токенов × 100 документов × 1000 запросов = $1000/день.
    
- **Медленно:** 2-5 секунд на batch.
    
- **Position bias:** Модель предпочитает документы, которые стоят первыми в промпте.
    

**Решение: Distillation**

1. Собираем датасет: запросы + документы + ранги от GPT-4.
    
2. Обучаем маленький Cross-Encoder (MiniLM, 33M параметров) предсказывать эти ранги.
    
3. Получаем 95% качества GPT-4 за 1% стоимости.
    

---



#### Вопрос: Когда использование LLM как reranker оправдано?
**Ответ:**
Когда релевантность требует рассуждения (multi‑hop), понимания сложных инструкций и контекста, или когда нужно ранжировать по «полезности» для ответа. В таких случаях LLM может лучше интерпретировать нюансы, чем классический reranker.
#### Вопрос: Как сделать LLM‑rerank масштабируемым?
**Ответ:**
Ограничивают число кандидатов, используют строгий формат промпта и компрессию контекста, батчат запросы, а главное — делают дистилляцию: собирают оценки/ранги LLM и обучают компактный cross‑encoder повторять их качество.



#### Вопрос: Почему reranking важен, даже если retrieval уже хороший?
**Ответ:**
Потому что бизнес‑и пользовательский эффект определяется верхушкой выдачи. Даже при высоком recall неправильный порядок топ‑10 снижает CTR/удовлетворенность. Reranker оптимизирует именно порядок кандидатов под NDCG/MRR/Precision.
#### Вопрос: Как обычно выбирают топ‑K кандидатов для reranking?
**Ответ:**
K выбирают по компромиссу latency vs качество: чем больше K, тем выше шанс включить нужный документ, но тем дороже rerank. На практике строят кривые Recall@K и NDCG@K, и подбирают K под SLO (p95/p99).

## 4. МЕТРИКИ КАЧЕСТВА (EVALUATION)

### 4.1. Offline Metrics[habr](https://habr.com/ru/articles/948786/)​

#### A. Binary Relevance

**Precision@K:**  
P@K=Relevant docs in top-KKP@K=KRelevant docs in top-K

- _Пример:_ Топ-10, из них 7 релевантны → $P@10 = 0.7$.
    

**Recall@K:**  
R@K=Relevant docs in top-KTotal relevant docsR@K=Total relevant docsRelevant docs in top-K

- _Пример:_ Всего 20 релевантных, в топ-10 нашли 7 → $R@10 = 0.35$.
    

**F1@K:** Гармоническое среднее Precision и Recall.

**MAP (Mean Average Precision):**  
Для каждого релевантного документа считаем Precision на его позиции, усредняем.  
AP=1∣Rel∣∑k=1NP@k⋅rel(k)AP=∣Rel∣1∑k=1NP@k⋅rel(k)  
Где $rel(k) = 1$, если документ на позиции $k$ релевантен.

---



#### Вопрос: Как интерпретировать Precision@K и Recall@K в контексте retrieval и rerank?
**Ответ:**
Precision@K говорит о «чистоте» топ‑K, recall@K — о покрытии релевантных документов в топ‑K. Для retrieval обычно важнее recall@100/1000, а для rerank — precision@10/20; смешение целей приводит к неправильной оптимизации.
#### Вопрос: Чем MAP отличается от Precision@K и когда MAP полезнее?
**Ответ:**
MAP учитывает позиции всех релевантных документов, усредняя precision в точках их появления. Он полезен, когда релевантных результатов много и важно качество по всему списку, а не только в фиксированном K.

#### B. Ranked Relevance (Порядок важен)

**MRR (Mean Reciprocal Rank):**  
MRR=1∣Q∣∑i=1∣Q∣1rankiMRR=∣Q∣1∑i=1∣Q∣ranki1  
Где $rank_i$ — позиция _первого_ релевантного документа для запроса $i$.

- _Используется:_ QA системы, где один правильный ответ (например, "Столица Франции?").
    
- _Интерпретация:_ Если первый правильный ответ на 2-м месте, вклад запроса = $0.5$. Если на 5-м → $0.2$.
    

**NDCG@K (Normalized Discounted Cumulative Gain)** [GOLD STANDARD]  
Учитывает:

1. Градации релевантности (не просто 0/1, а 0-5).
    
2. Позицию (ошибка на 1-м месте "дороже", чем на 10-м).
    

DCG@K=∑i=1K2reli−1log⁡2(i+1)DCG@K=∑i=1Klog2(i+1)2reli−1  
NDCG@K=DCG@KIDCG@KNDCG@K=IDCG@KDCG@K  
Где $IDCG$ — идеальный DCG (если бы документы были отсортированы идеально).

- _Пример:_ Релевантности на позициях.
    
- $DCG = \frac{2^3-1}{\log_2(2)} + \frac{2^2-1}{\log_2(3)} + ... = 7 + 1.89 + 4.42 + 0 + 0.38 = 13.69$
    
- $IDCG$ (если бы порядок был ) = 15.3
    
- $NDCG = 13.69 / 15.3 = 0.895$
    

---



#### Вопрос: Почему MRR особенно подходит для QA‑сценариев?
**Ответ:**
В QA часто важен один правильный документ/ответ, и пользователь ожидает увидеть его как можно выше. MRR прямо измеряет позицию первого релевантного результата, поэтому хорошо отражает успех таких систем.
#### Вопрос: Почему NDCG считается «золотым стандартом» для ранжирования?
**Ответ:**
NDCG учитывает и позицию (discount: выше важнее), и уровни релевантности (gain), а нормализация на идеальную выдачу делает значения сопоставимыми между запросами. Это хорошо совпадает с тем, как пользователи потребляют выдачу.



#### Вопрос: Когда использовать бинарную релевантность, а когда градуированную (0–3/0–5)?
**Ответ:**
Бинарная подходит, когда критерий «подходит/не подходит» строгий. Градуированная полезна, когда есть разные уровни полезности (идеальный/частичный), и важно, чтобы «самые полезные» были выше — тогда NDCG отражает качество точнее.
#### Вопрос: Какие типичные ошибки при построении офлайн‑датасета для evaluation?
**Ответ:**
Смещение логов (position bias), неполная разметка, утечка train→test и несоответствие запросов реальному трафику. Это ведет к завышенным офлайн метрикам и неожиданным провалам в продакшене.

### 4.2. Online Metrics (Production)

**CTR (Click-Through Rate):**  
CTR=ClicksImpressionsCTR=ImpressionsClicks  
Доля запросов, где пользователь кликнул на результат.

**Dwell Time:**  
Среднее время, проведенное на странице результата. Proxy для "нашел ли пользователь ответ".

**Zero Results Rate:**  
ZRR=Queries with 0 resultsTotal queriesZRR=Total queriesQueries with 0 results  
Критическая метрика. Если ZRR > 10%, нужен Query Expansion или Spell Correction.

**SERP Abandonment:**  
Пользователь не кликнул ни на один результат (возможно, нашел ответ в сниппете, или ничего не подошло).

---



#### Вопрос: Почему CTR нельзя считать прямой метрикой релевантности?
**Ответ:**
CTR зависит от позиции, сниппета, дизайна SERP и привычек пользователя. Можно поднять CTR за счет кликабельности, не улучшив полезность. Поэтому CTR нужно дополнять dwell time, конверсией и анализом abandonment.
#### Вопрос: Что диагностируют Zero Results Rate и SERP Abandonment и почему это разные сигналы?
**Ответ:**
ZRR показывает случаи, когда система вернула 0 результатов (проблемы индекса/фильтров/понимания запроса). Abandonment показывает, что результаты были, но пользователь не кликнул (возможна низкая релевантность или ответ в сниппете). Вместе они помогают локализовать проблему.



#### Вопрос: Почему нельзя оценивать поисковую систему одной метрикой?
**Ответ:**
Потому что разные компоненты и задачи требуют разных целей: retrieval — Recall@K, ранжирование — NDCG/MRR, продукт — CTR/Revenue. Одна метрика не покажет, где именно провал: в покрытии кандидатов, в порядке топа или в поведении пользователей.
#### Вопрос: Почему офлайн‑метрики и онлайн‑метрики могут расходиться?
**Ответ:**
Офлайн меряет релевантность по разметке/логам, онлайн — реальное поведение, зависящее от UI, сниппета и position bias. Поэтому улучшение NDCG не гарантирует рост CTR без A/B, и наоборот.

## 5. ADVANCED TOPICS (ПРОДВИНУТЫЕ ТЕХНИКИ)

### 5.1. Hard Negatives Mining

**Проблема:** Если обучать Bi-Encoder на случайных негативах (берем random документы из корпуса), модель быстро переобучается. Она учится отличать "очевидно неподходящие" документы, но не учится тонким различиям.[deepschool](https://blog.deepschool.ru/llm/rag-ot-pervoj-versii-k-rabochemu-resheniyu/)​

**Hard Negative:** Документ, который:

- Похож на запрос (высокий BM25 score или cosine similarity).
    
- Но **НЕ релевантен** (нет в ground truth).
    

**Методы майнинга:**

1. **In-batch Negatives:** Другие документы в батче (если батч = 32 пары (q, d+), то для запроса $q_1$ негативами будут $d_2, ..., d_{32}$).
    
2. **BM25 Negatives:** Берем топ-100 по BM25, исключаем ground truth.
    
3. **ANN Negatives:** Используем текущую версию модели, ищем ближайших соседей к запросу, исключаем ground truth.
    

**Результат:** Качество модели (NDCG) растет на 5-15%.

---



#### Вопрос: Почему hard negatives важнее случайных негативов для обучения bi‑encoder?
**Ответ:**
Случайные негативы слишком легкие и не учат модель различать близкие по смыслу документы. Hard negatives имитируют реальные «конкурирующие» кандидаты в retrieval и заставляют модель учиться тонким различиям, повышая качество top‑K.
#### Вопрос: Как practically получать hard negatives в поисковом проекте?
**Ответ:**
Часто берут топ‑K кандидатов из BM25 или текущего ANN‑retrieval, удаляют истинные позитивы и используют оставшееся как hard negatives. Процесс делают итеративным: улучшили модель → добыли более сложные негативы → дообучили снова.

### 5.2. Query Understanding

Перед тем как искать, нужно понять запрос.

**Spell Correction:**

- "айфон 15 про" → "iPhone 15 Pro"
    
- Методы: Levenshtein distance + frequency dictionary, Neural Spell Checker (T5-based).
    

**Query Segmentation:**

- "купить ноутбук asus 16gb" → `[action: купить] [category: ноутбук] [brand: asus] [spec: 16gb]`
    
- Используется для фильтров (в БД ищем только ноутбуки ASUS с RAM 16GB).
    

**Intent Classification:**

- _Navigational:_ Пользователь ищет конкретный сайт ("Apple официальный сайт").
    
- _Informational:_ Хочет узнать факт ("Что такое трансформер?").
    
- _Transactional:_ Готов купить ("купить iPhone 15").
    

---



#### Вопрос: Почему query understanding — это не только spell correction, но и сегментация/интент?
**Ответ:**
Исправление опечаток повышает recall, но для продакшена важна структура запроса: выделить бренд/категорию/атрибуты, понять интент (инфо/транзакция/навигация) и применить корректные фильтры/вертикаль. Это повышает управляемость и качество.
#### Вопрос: Как query segmentation помогает сочетать поиск с каталогом и фильтрами?
**Ответ:**
Сегментация превращает строку в атрибуты (brand, category, характеристики), что позволяет заранее отфильтровать кандидатов и искать в правильном подпространстве. Затем BM25/dense/LTR ранжируют уже среди релевантного набора, снижая шум.

### 5.3. Multi-modal Retrieval

**Задача:** Запрос может быть текст + картинка, а документы могут содержать видео + текст.

**Архитектура (CLIP-based):**

1. Image Encoder (Vision Transformer) → $\vec{v}_{img}$
    
2. Text Encoder (BERT) → $\vec{v}_{text}$
    
3. Проекция в общее пространство (размерность 512).
    
4. Поиск: $CosSim(\vec{v}_{query}, \vec{v}_{doc})$
    

**Примеры:**

- Поиск товара по фото (обратный поиск).
    
- "Покажи мне кроссовки как на этой картинке, но красные" (hybrid query).
    

---



#### Вопрос: Что означает «общее пространство» для текста и изображения в мультимодальном поиске?
**Ответ:**
Это когда text encoder и image encoder обучены так, что описывающие одно и то же объекты оказываются близко в одном векторном пространстве (как в CLIP). Тогда можно искать картинку по тексту и наоборот, используя стандартный ANN по векторам.
#### Вопрос: Какие инженерные сложности типичны для мультимодального поиска товаров?
**Ответ:**
Фон/ракурс/качество фото могут доминировать над содержанием, а атрибуты (цвет, размер) нужно учитывать отдельно. Часто используют гибрид: визуальный retrieval + текстовые фильтры/условия или rerank, чтобы результаты были и похожими, и удовлетворяли ограничениям запроса.

### 5.4. RAG-Specific: Chunk Strategy[deepschool](https://blog.deepschool.ru/llm/rag-ot-pervoj-versii-k-rabochemu-resheniyu/)​

**Проблема:** Документ в 10k токенов не влезет в эмбеддинг. Нужно разрезать (chunking).

**Методы:**

1. **Fixed-size:** 512 токенов, overlap 50. Просто, но может разорвать смысловой блок.
    
2. **Semantic Chunking:** Режем по параграфам. Если embedding distance между параграфами скачет (> threshold), это граница чанка.
    
3. **Hierarchical (Parent-Child):**
    
    - Храним мелкие чанки (128 токенов) для поиска.
        
    - При retrieval возвращаем "родительский" чанк (512 токенов) для контекста LLM.
        

---



#### Вопрос: Почему chunk strategy критична для качества RAG даже при хорошем retrieval?
**Ответ:**
Если чанки режут смысловые блоки, retrieval может вернуть фрагмент без ключевого контекста или пропустить нужный факт. Тогда LLM видит неполную информацию и чаще ошибается. Хороший chunking повышает шанс получить связный, полезный контекст.
#### Вопрос: Когда фиксированные чанки лучше семантических, и наоборот?
**Ответ:**
Фиксированные проще и стабильнее, полезны при равномерных текстах и строгих лимитах. Семантические/иерархические лучше для документации и статей, где важно не разрывать определения и связанный контекст, улучшая и recall, и читабельность контекста для LLM.

### 5.5. Context Compression для RAG

**Проблема:** Топ-10 чанков по 500 токенов = 5000 токенов контекста. Это дорого и может не влезть в окно LLM.[deepschool](https://blog.deepschool.ru/llm/rag-ot-pervoj-versii-k-rabochemu-resheniyu/)​

**Решения:**

1. **LongLLMLingua:** Модель-компрессор. Удаляет "лишние" токены, сохраняя суть.
    
    - "The company was founded in 1998 in California" → "Founded 1998 California".
        
2. **Extractive Summarization:** BERT-based модель выделяет key sentences.
    
3. **Reranking Context:** Cross-Encoder сортирует чанки, берем топ-3 вместо топ-10.
    

---



#### Вопрос: Почему компрессия контекста может повысить качество ответа, а не только снизить стоимость?
**Ответ:**
Она удаляет шум и оставляет ключевые факты в пределах окна LLM, уменьшая конкуренцию фрагментов и риск противоречий. В итоге LLM легче «увидеть» нужный сигнал и ответить точнее.
#### Вопрос: Как правильно сочетать reranking и compression в RAG?
**Ответ:**
Обычно сначала делают rerank (cross‑encoder/ColBERT) чтобы выбрать самые релевантные чанки, а затем компрессируют именно их. Компрессия «всего топ‑10» без сильного rerank часто тратит токены на нерелевантное и ухудшает итог.



#### Вопрос: Какие «продвинутые» улучшения чаще всего дают прирост без смены модели на более крупную?
**Ответ:**
Hard negatives mining, query understanding, хороший chunking и context compression часто устраняют основные источники ошибок пайплайна. Это может дать больший эффект, чем переход на «чуть более SOTA» модель без изменения данных и процесса.
#### Вопрос: Почему advanced‑компоненты особенно важны в RAG?
**Ответ:**
Потому что качество RAG зависит от того, какие чанки попадут в контекст LLM и насколько они чистые. Ошибки retrieval и чанков быстро превращаются в галлюцинации и неточные ответы, поэтому улучшения в retrieval, chunk strategy и compression напрямую повышают конечное качество.

## 6. PRODUCTION & SCALABILITY

### 6.1. Vector Databases (Сравнение)

|Параметр|Qdrant|Milvus|Weaviate|Pinecone|
|---|---|---|---|---|
|**Язык**|Rust|C++/Go|Go|Managed (Python SDK)|
|**ANN**|HNSW|HNSW/IVF/PQ|HNSW|Проприетарный|
|**Гибридный поиск**|✅ (Sparse + Dense)|❌ (только Dense)|✅|❌|
|**Фильтрация**|✅ (по метаданным)|✅|✅|✅|
|**Масштабирование**|Vertical + Horizontal|Distributed (Kubernetes)|Horizontal|Auto|
|**Стоимость**|Open-source / Cloud|Open-source / Cloud|Open-source / Cloud|Дорого ($70+/мес)|

**Выбор:**

- Qdrant — если нужен гибридный поиск и фильтрация.
    
- Milvus — для огромных масштабов (> 1B векторов).
    
- Pinecone — если бюджет не ограничен и нужна простота.
    

---



#### Вопрос: Почему поддержка фильтрации по метаданным важна для векторных БД?
**Ответ:**
Без фильтров dense retrieval может тянуть «семантически похожее» из неправильной версии/категории, что снижает точность. Метаданные позволяют ограничить поиск (язык, версия, раздел) и одновременно ускорить retrieval за счет меньшего пространства поиска.
#### Вопрос: Как читать сравнение векторных БД при выборе под продакшен?
**Ответ:**
Смотрите на: поддерживаемые индексы (HNSW/IVF/PQ), качество фильтров, гибридный поиск, шардирование/репликацию и операционные требования (self-host vs managed). Выбор часто определяется не «чьё быстрее на бенче», а тем, как вы будете обновлять индекс и держать SLO.

### 6.2. Latency Optimization

**1. Model Quantization:**

- FP32 → INT8: скорость ×4, качество -1%.
    
- ONNX Runtime поддерживает квантизацию из коробки.
    

**2. Batch Inference:**

- Вместо обработки запросов по одному, группируем в батчи (8-32).
    
- GPU utilization растет с 20% до 80%.
    

**3. Cached Embeddings:**

- Для популярных запросов храним предпосчитанные эмбеддинги в Redis.
    

**4. Model Distillation:**

- BERT-large (340M) → DistilBERT (66M): скорость ×2, качество -3%.
    

---



#### Вопрос: Какая оптимизация обычно дает быстрый выигрыш в latency без смены модели?
**Ответ:**
Кэширование популярных запросов (и их эмбеддингов/результатов) часто дает самый быстрый эффект по p95. Затем батчинг повышает утилизацию GPU, а ONNX/квантизация ускоряют инференс, особенно для reranker.
#### Вопрос: Почему дистилляция — ключевой инструмент для ускорения reranking?
**Ответ:**
Потому что reranking выполняется онлайн и напрямую влияет на latency. Дистилляция позволяет взять сильного учителя (LLM или большой cross‑encoder) и обучить маленькую модель повторять ранги/скоры, сохраняя большую часть качества при кратно меньшей стоимости.



#### Вопрос: Почему масштабирование retrieval — это не только про размер индекса, но и про обновления?
**Ответ:**
В продакшене документы добавляются и меняются, и индекс должен обновляться без долгих стопов и деградации качества. Поэтому важны стратегии инкрементального апдейта, шардирование, репликация и согласованность метаданных/фильтров.
#### Вопрос: Какие оптимизации чаще всего критичны для p95 latency в поиске?
**Ответ:**
Параметры ANN (ef/nprobe), кэширование эмбеддингов запросов, батчинг на GPU, оптимизация инференса (ONNX/квантизация), сокращение сетевых hops к векторной БД. Часто именно инфраструктура определяет, уложится ли система в SLO.

## ИТОГОВАЯ СВОДКА (CHEAT SHEET)

### Когда что использовать?

|Задача|Метод Stage 1|Метод Stage 2|Почему?|
|---|---|---|---|
|**FAQ / RAG (малый корпус < 10k)**|BM25 + SBERT|Cross-Encoder|Простота, высокая точность [jina](https://jina.ai/ru/news/maximizing-search-relevancy-and-rag-accuracy-with-jina-reranker/)​|
|**Поиск по базе знаний (> 1M docs)**|BGE Bi-Encoder + HNSW|ColBERT|Баланс скорость/точность|
|**E-commerce (артикулы, точные совпадения)**|BM25|LambdaMART (LTR)|Keyword matching важен [deepschool](https://blog.deepschool.ru/llm/rag-ot-pervoj-versii-k-rabochemu-resheniyu/)​|
|**Семантический поиск (синонимы, перефразы)**|E5 Bi-Encoder|Cross-Encoder|Dense Retrieval сильнее|
|**Огромный масштаб (> 100M docs)**|Faiss IVF-PQ|Mini-Cross-Encoder (distilled)|Компрессия + скорость|



#### Вопрос: Почему для e‑commerce с артикулами часто предлагают BM25 + LTR как базовую связку?
**Ответ:**
Артикулы и названия дают сильный лексический сигнал, поэтому BM25 дает высокий precision на топе. LTR добавляет бизнес‑сигналы (популярность, наличие, цена, маржинальность) и доводит порядок под цели продукта.
#### Вопрос: Почему для больших баз знаний часто выбирают dense retrieval + ANN, а не только BM25?
**Ответ:**
Запросы пользователей часто формулируются как вопросы и перефразы, где важна семантика. Dense retrieval через bi‑encoder + ANN повышает recall на таких запросах, а дальше reranker (ColBERT/cross‑encoder) обеспечивает точный топ‑K.

### Метрики для разных задач

|Задача|Ключевая метрика|Почему?|
|---|---|---|
|QA (один правильный ответ)|MRR|Важна позиция первого правильного|
|Поиск (несколько релевантных)|NDCG@10|Учитывает порядок и градации [habr](https://habr.com/ru/articles/948786/)​|
|RAG (нужны все факты)|Recall@K|Пропуск факта → галлюцинация LLM|
|E-commerce (продажи)|CTR, Revenue per Search|Бизнес-метрики важнее оффлайн|

---



#### Вопрос: Почему для RAG ключевой метрикой считают Recall@K, а не NDCG@10?
**Ответ:**
Если нужный факт не попал в retrieved чанки, LLM не сможет на него опереться. Поэтому сначала оптимизируют покрытие (Recall@K/coverage), а затем порядок и компрессию контекста для улучшения качества ответа.
#### Вопрос: Почему для e‑commerce в метриках важны CTR и Revenue per Search?
**Ответ:**
Потому что конечная цель — клики и покупки. Офлайн метрики важны для быстрого тюнинга, но успех продукта определяется онлайн‑поведением и выручкой, поэтому эти метрики становятся основными для принятия решений.

### ЗАКЛЮЧЕНИЕ

Современный поиск — это сложная инженерная система, где каждый компонент решает свою подзадачу. BM25 не умер — он дополняет нейросетевые методы в гибридном поиске. Cross-Encoders дают максимальную точность, но применяются только на финальной стадии. ColBERT показывает, что можно найти "золотую середину" между скоростью Bi-Encoder и точностью Cross-Encoder.[jina+1](https://jina.ai/ru/news/maximizing-search-relevancy-and-rag-accuracy-with-jina-reranker/)​

Для продакшена критичны не только алгоритмы, но и инфраструктура: векторные базы, квантизация моделей, кеширование. A/B тесты показывают, что улучшение NDCG на 2% может дать рост CTR на 10%.

**Главный вывод:** Не существует "одного лучшего метода". Правильная архитектура — это гибридный пайплайн, где каждый этап оптимизирован под свои метрики и constraints.

#### Вопрос: Какая минимальная «правильная» архитектура следует из документа для современного продакшена?
**Ответ:**
Двухстадийный пайплайн: Stage 1 (BM25 и/или bi‑encoder + ANN, возможно hybrid) для высокого recall; Stage 2 (cross‑encoder/ColBERT/LTR/иногда LLM) для качества топ‑K. Плюс измерение офлайн/онлайн метрик и оптимизации (кэш, батчинг, квантизация, дистилляция).
#### Вопрос: С чего обычно начать улучшение системы после базового baseline, чтобы получить максимальную отдачу?
**Ответ:**
Чаще всего — поднять recall на retrieval (hybrid, hard negatives, фильтры), затем улучшить порядок топа (reranker/LTR), затем оптимизировать chunking и compression для RAG и параллельно снижать latency (кэш, ANN‑параметры, батчинг, дистилляция).



#### Вопрос: Как использовать сводку, чтобы быстро выбрать архитектуру под свой масштаб и цель?
**Ответ:**
Сначала определите цель (RAG/QA/e‑commerce) и constraint по latency, затем выберите Stage 1 (BM25/dense+ANN/hybrid) и Stage 2 (cross‑encoder/ColBERT/LTR) по таблице. После этого задайте метрики (Recall@K, NDCG/MRR, CTR) и план экспериментов.
#### Вопрос: Почему «нет одного лучшего метода» — практический вывод для продакшена?
**Ответ:**
Потому что методы оптимизированы под разные режимы: BM25 для точных термов, dense для перефраз, cross‑encoder для точности на малом K, IVF‑PQ для масштаба, LTR для смешивания сигналов. Поэтому выигрыш дает композиция методов и правильный тюнинг под данные и SLO.

